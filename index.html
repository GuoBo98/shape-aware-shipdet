<!doctype html>
<html>
<head>

<<<<<<< HEAD
	<title>Spape-Aware-Shipdet</title>
=======
	<title>Shape-Aware-Shipdet</title>
	<meta name="viewport" content="user-scalable=no, initial-scale=1, maximum-scale=1, minimum-scale=1">
	<link href="css/main.css" media="screen" rel="stylesheet" type="text/css"/>
	<link href="css/index.css" media="screen" rel="stylesheet" type="text/css"/>
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:400,700' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Raleway:400,600,700' rel='stylesheet' type='text/css'>
	<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
			CommonHTML: { linebreaks: { automatic: true } },
			"HTML-CSS": { linebreaks: { automatic: true } },
				 SVG: { linebreaks: { automatic: true } }
			});
	</script>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	
</head>

<body>

<div class="menu-container noselect">
	<div class="menu">
		<table class="menu-table">
			<tr>
				<td>
					<div class="logo">
						<a href="javascript:void(0)">Shape-Aware-Shipdet</a>
					</div>
				</td>
				<td>
					<div class="menu-items">
						<a class="menu-highlight">Overview</a>
						<a href="https://github.com/ZhangRuixiang-WHU/Shape-Aware-Shipdet">GitHub</a>
					</div>
				</td>
			</tr>
		</table>
	</div>
</div>

<div class="content-container">
	<div class="content">
		<table class="content-table">
		      <h1 style="text-align:center; margin-top:60px; font-weight: bold; font-size: 35px;">
				Fine-Grained Ship Detection in High-Resolution Remote Sensing Images with Shape-Aware Feature Learning </h1>
	        
				<p style="text-align:center; margin-bottom:15px; margin-top:20px; font-size: 18px;">
					<a href="https://github.com/GuoBo98/" style="color: #0088CC">Bo Guo<script type="math/tex"></script>, </a>
					<a href="https://github.com/ZhangRuixiang-WHU/" style="color: #0088CC">Ruixiang Zhang<script type="math/tex"></script>, </a>
					<a href="https://github.com/whughw" style="color: #0088CC">Haowen Guo<script type="math/tex"></script>, </a>
					<a href="http://www.captain-whu.com/yangwen_En.html" style="color: #0088CC">Wen Yang<script type="math/tex">^{*}</script>, </a>
          				<a href="https://github.com/levenberg" style="color: #0088CC">Huai Yu<script type="math/tex"></script>,</a>
					<!-- <a href="http://www.dsi.unive.it/~pelillo/" style="color: #0088CC">Marcello Pelillo<script type="math/tex">^3</script></a> <br> -->
    
					
					
					<a target="_blank" href="https://dsp.whu.edu.cn/" style="color: #0088CC; font-style: italic"><script type="math/tex"></script>EIS SPL, Wuhan University, Wuhan, China</a><br>
		

				</p>	 
				<p style="text-align:center; margin-bottom:15px; margin-top:20px; font-size: 18px;"> 
					<a href="https://github.com/GuoBo98/Shape-Aware-Shipdet" style="color: hsl(17, 100%, 40%)"> Data and code coming soon!
					</a>
				</p>
		
			
			<tr>
				<!-- <td colspan="1"> -->
					<h2 class="add-top-margin">Abstract</h2>
					<hr>
				<!-- </td> -->
			</tr>
			<tr>
					<!-- <td colspan="1"> -->

				<p class="text" style="text-align:justify;">
					Fine-grained ship detection is a significant task in high-resolution remote sensing images. 
					However, large aspect ratios and severe category imbalance make fine-grained ship detection a challenging task. 
					Current general feature extraction methods are not entirely suitable for ships with large aspect ratios, 
					which will lead to misalignments in feature extraction, affecting the succeeding localization and classification. 
					For this problem, we propose a shape-aware feature learning method to fix the misalignments during feature extraction. 
					And For the problem of category imbalance, we design a ship shape-aware instance switching to balance the quantity distribution 
					of ships in different categories. It can ensure the consistency of the ocean background and ship direction when replacing 
					common ship instances with rare ship instances, which can improve the network's learning ability for rare instances. To verify 
					the effectiveness of the proposed method, we construct a multi-category ship detection dataset (MCSD) that contains 4000 images 
					carefully labeled with oriented bounding boxes, including 16 types of ship objects and nearly 18,000 instances. 
					We conduct experiments on MCSD and ShipRSImageNet, and extensive experiments indicate our proposed method's superiority for 
					ship detection.
				</p>
					<!-- </td> -->
			</tr>

			<tr>
				<!-- <td colspan="1"> -->
					<h2 class="add-top-margin">Introduction</h2>
					<hr>
				<!-- </td> -->
			</tr>

			<tr>
				<p class="text" style="text-align:justify;">
					Fine-grained ship detection in high-resolution remote sensing images is important for marine monitoring and management, 
					maritime rescue and coastal military defense early warning, etc. 
					In recent years, the resolution of remote sensing images is increasing with the development of earth observation technology, 
					which provides strong support for fine-grained ship detection and recognition.
					

          <br>
					Benefiting from the powerful feature representation of deep convolutional neural network (DCNN), object detection algorithms based on DCNN, 
					such as Faster R-CNN, YOLO, and SSD have achieved excellent detection results in natural scene images. 
					However, remote sensing images are different from natural scene images in terms of imaging angle and resolution. The massive variations in the scale and orientation 
					of objects caused by the bird's-eye view bring challenges to remote sensing object detection. As the essential target in optical remote sensing 
					images, ship has the characteristics of arbitrary direction, large aspect ratio, and multiple categories, which lead to the following problems for ship detection in 
					remote sensing images.
 
          <br>
          <br>
            However, detecting the oil tanks in SAR images is very challenging due to the unique imaging mechanism of SAR images. 
			The oil tank targets in the SAR images are usually dense and compact with more overlaps and discrete scattering centers, which increases the difficulty of location in detection. 
			Figure. 1 shows some samples of SAR images with oil tanks. 
			As we can see, the heavy scattering interference between the dense oil tanks causes a blurred and cluttered appearance, which is hard to distinguish even the experts of SAR image analysis.
				</p>
			</tr>

			<tr>
				<td>
					<div>
					<img class="center" src="./images/fig1.png" width="800" /> <br>
					<p class="image-caption">
						Figure 1. (a) is the high-resolution SAR image with compact oil tanks. The SAR Intensity is visualized with three polarizations (HH, VV, HV) displayed through the Red, Green, and Blue color channels. (b) is the corresponding optical image. (c) is the detection result of Faster R-CNN. (d) is the detection result of our method. (The green boxes refer to the correct detection results, the yellow boxes refer to the false detection results, and the red boxes refer to the missing targets.
					</p>				
					</div>
				</td>
			</tr>

			<tr>
				<p class="text" style="text-align:justify;">
					For the existing methods, the unsatisfactory detection accuracy is mainly caused by the indistinguishable appearance of the oil tanks in  high-resolution SAR images. 
					So naturally, how can we teach the network learning to recognize the oil tank in SAR images? 
					As we all know, when humans encounter an illegible image of an unknown modality, 
					we often learn to recognize it by comparing the corresponding image of a recognizable modality. 
					Inspired by that, we propose introducing optical images more suitable for human vision to guide the network to learn the feature of oil tanks in SAR images. 
				</p>
			</tr>

			<tr>
				<td>
					<div>
					<img class="center" src="./images/fig2.png" width="800" /> <br>
					<p class="image-caption">
						Figure 2. The proposed pipeline for detecting oil tanks in SAR images, uses optical-SAR self-supervised learning to pre-train a better feature extractor as the initialization in the training stage. First, the optical network guides the SAR network as the teacher. Then, the model detects the oil tanks during inference using only SAR images as input.
					</p>				
					</div>
				</td>
			</tr>
			
			<tr>
				<td>
				<p class="text" style="text-align:justify;">
					  Based on the above, we design a multi-stage pipeline for detecting oil tanks in SAR images with the enhancement of optical images, as shown in Figure. 2. 
					  First, Optical-guided self-supervised learning is used in the pre-training stage to obtain the feature extractor suitable for SAR images. 
					  Then, in the training stage,  a teacher-student network is constructed to make the SAR branch learn how to recognize oil tanks from the optical branch through knowledge distillation. 
					  Finally, the model detects the oil tanks using only SAR images as input during the inference stage. Our contribution can be summarized as follows:
					
					<ul style="font-weight:normal; text-align:justify;">
							<li> We propose a pipeline that uses optical images to guide SAR images to train a better oil tank detector in SAR images. Paired optical images and SAR images are used in the training stage, while only SAR images are used in the inference stage. </li>
							<li> We introduce paired optical-SAR images into the self-supervised learning to obtain a better SAR image feature extractor, improving the downstream oil tank detection task by 8-9 %. </li>
							<li> We construct an oil tank detection dataset, Shape-Aware-Shipdet, to explore the effectiveness of the proposed method. The dataset contains 820 paired SAR-Optical images with carefully labeled bounding boxes.</li>							
					</ul>

				</p>
				</td>
			</tr>
      <!-- <tr>
				<td>
						<h2 class="add-top-margin">AI-TOD-v2</h2>
						<hr>
				</td>
			</tr>
      <tr>
        <td>
            <h3 class="add-top-margin">Statistics</h3>
            
        </td>
    </tr>

    <tr>
      <td>
        <div>
        <img class="center" src="images/statistics.PNG" width="1000" /> <br>
        <p class="image-caption">
          Figure 2. Statistics of classes and instances in AI-TOD-v2. (a) Histogram of the number of instances per class. (b) Histogram of number of instances per image. (c) Histogram of number of instances’ sizes. (d) Boxplot depicting the range of sizes for each object category.
        </p>				
        </div>
      </td>
    </tr>

	<tr>
		<td>
		  <div>
		  <img class="center" src="images/mean.PNG" width="500" /> <br>
		  <p class="image-caption">
			Table 1. Mean and standard deviation of object scale on different
			datasets.
		  </p>				
		  </div>
		</td>
	  </tr>

	  <tr>
        <td>
            <h3 class="add-top-margin">Download</h3>
            1. AI-TOD
			[<a href="https://drive.google.com/drive/folders/1mokzFtLCjyqalSEajYTUmyzXvOHAa4WX" style="color: #0088CC">images & annotations</a>]<br>
			2. AI-TOD-v2
			[<a href="https://drive.google.com/drive/folders/1mokzFtLCjyqalSEajYTUmyzXvOHAa4WX" style="color: #0088CC">images & annotations</a>]<br>
			Note that the AI-TOD and AI-TOD-v2 share the image sets.
        </td>
     </tr> -->

			<tr>
				<td>
						<h2 class="add-top-margin">Experimental Results</h2>
						<hr>
				</td>
			</tr>

			<tr>
					<td>
							<h3 class="add-top-margin">A Comparison of Different Methods on Shape-Aware-Shipdet</h3>
							
					</td>
			</tr>
					
			<tr>
				<td>
					<div>
					<img class="center" src="images/fig3.png" width="1000" /> <br>
					<p class="image-caption">
						Figure 3. Visualization of detection results using our proposed method compared with other state-of-the-art methods. The green boxes refer to the correct detection results, the yellow boxes refer to the false detection results, and the red boxes refer to the missing targets.
					</p>				
					</div>
				</td>
			</tr>

			<!-- <tr>
				<td>
						<h2 class="add-top-margin">Related Work</h2>
						<hr>
						<p class="text" style="text-align:justify;"></p>
						Our work is built based on our past works including: <a href="https://ieeexplore.ieee.org/abstract/document/9413340" style="color: #0088CC">AI-TOD</a> [1], <a href="https://arxiv.org/abs/2102.12219" style="color: #0088CC">DOTA-v2.0</a> [3], <a href="https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Xu_Dot_Distance_for_Tiny_Object_Detection_in_Aerial_Images_CVPRW_2021_paper.html" style="color: #0088CC">DotD</a> [4] and <a href="https://arxiv.org/abs/2110.13389" style="color: #0088CC">NWD</a> [5]. If you found our work helpful, consider citing these works as well.
				</td>
			</tr> -->
			<tr>
					<td>
							<h2 class="add-top-margin">Acknowledgements</h2>
							<hr>
							<p class="text" style="text-align:justify;"></p>
						We would like to thank the anonymous reviewers for their valuable comments and contributions. 
						This project has received funding from the National Natural Science Foundation of China (NSFC) under Grant 61771351. 
						In addition, the numerical calculations in this article have been done on the supercomputing system in the Supercomputing Center, Wuhan University.
							
					</td>
				</tr>

			<tr>
					<td colspan="2">
					<h2 class="add-top-margin">References</h2>
					<hr>
					<ol style="padding-inline-start:20px;">
						<li>
							<b>Spacenet 6: Multi-sensor all weather mapping dataset </b>
							[<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w11/Shermeyer_SpaceNet_6_Multi-Sensor_All_Weather_Mapping_Dataset_CVPRW_2020_paper.pdf" style="color: #0088CC">paper</a>]<br>
								J. Shermeyer, D. Hogan, J. Brown, A. Van Etten, N. Weir,
								F. Pacifici, R. Hansch, A. Bastidas, S. Soenen, T. Bacastow
							et al. <br>
								IEEE Conference on Computer Vision and Pattern Recognition Workshops (<b>CVPRW</b>), 2020.
						</li>
						<li>
							<b>Bootstrap your own latent-a new approach to self-supervised learning </b>
							[<a href="https://proceedings.neurips.cc/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf" style="color: #0088CC">paper</a>]<br>
								J.-B. Grill, F. Strub, F. Altch ́e, C. Tallec, P. Richemond,
								E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al. <br>
								Advances in Neural Information Processing Systems (<b>NIPS</b>), 2020.
						</li>	
						<li>
							<b>EO-augmented building segmentation for airborne sar imagery </b>
							[<a href="https://ieeexplore.ieee.org/abstract/document/9417203" style="color: #0088CC">paper</a>]<br>
								J. Kim, S. Shin, S. Kim, and Y. Kim. <br>
								IEEE Geoscience and Remote Sensing Letters (<b>GRSL</b>), 2021.
						</li>	

						<li>
							<b>Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer </b>
							[<a href="https://arxiv.org/pdf/1612.03928.pdf" style="color: #0088CC">paper</a>]<br>
							N. Komodakis and S. Zagoruyko <br>
							International Conference on Learning Representations (<b>ICLR</b>), 2017.
						</li>	

						

					</ol>
					</td>
				</tr>
				
					
			<br><br>
		 </table>
		 			
		
	<div class="footer">
		<p class="block">&copy; 2021 by Chang Xu at SPL</p>
	</div>

	</div>
</div>
</body>
</html>

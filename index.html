<!doctype html>
<html>
<head>

	<title>Spape-Aware-Shipdet</title>
	<title>Shape-Aware-Shipdet</title>
	<meta name="viewport" content="user-scalable=no, initial-scale=1, maximum-scale=1, minimum-scale=1">
	<link href="css/main.css" media="screen" rel="stylesheet" type="text/css"/>
	<link href="css/index.css" media="screen" rel="stylesheet" type="text/css"/>
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:400,700' rel='stylesheet' type='text/css'>
	<link href='https://fonts.googleapis.com/css?family=Raleway:400,600,700' rel='stylesheet' type='text/css'>
	<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
			CommonHTML: { linebreaks: { automatic: true } },
			"HTML-CSS": { linebreaks: { automatic: true } },
				 SVG: { linebreaks: { automatic: true } }
			});
	</script>
	<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
	
</head>

<body>

<div class="menu-container noselect">
	<div class="menu">
		<table class="menu-table">
			<tr>
				<td>
					<div class="logo">
						<a href="javascript:void(0)">Shape-Aware-Shipdet</a>
					</div>
				</td>
				<td>
					<div class="menu-items">
						<a class="menu-highlight">Overview</a>
						<a href="https://github.com/ZhangRuixiang-WHU/Shape-Aware-Shipdet">GitHub</a>
					</div>
				</td>
			</tr>
		</table>
	</div>
</div>

<div class="content-container">
	<div class="content">
		<table class="content-table">
		      <h1 style="text-align:center; margin-top:60px; font-weight: bold; font-size: 35px;">
				Fine-Grained Ship Detection in High-Resolution Remote Sensing Images with Shape-Aware Feature Learning </h1>
	        
				<p style="text-align:center; margin-bottom:15px; margin-top:20px; font-size: 18px;">
					<a href="https://github.com/GuoBo98/" style="color: #0088CC">Bo Guo<script type="math/tex"></script>, </a>
					<a href="https://github.com/ZhangRuixiang-WHU/" style="color: #0088CC">Ruixiang Zhang<script type="math/tex"></script>, </a>
					<a href="https://github.com/whughw" style="color: #0088CC">Haowen Guo<script type="math/tex"></script>, </a>
					<a href="http://www.captain-whu.com/yangwen_En.html" style="color: #0088CC">Wen Yang<script type="math/tex">^{*}</script>, </a>
          				<a href="https://github.com/levenberg" style="color: #0088CC">Huai Yu<script type="math/tex"></script>,</a>
					<!-- <a href="http://www.dsi.unive.it/~pelillo/" style="color: #0088CC">Marcello Pelillo<script type="math/tex">^3</script></a> <br> -->
    
					
					
					<a target="_blank" href="https://dsp.whu.edu.cn/" style="color: #0088CC; font-style: italic"><script type="math/tex"></script>EIS SPL, Wuhan University, Wuhan, China</a><br>
		

				</p>	 
				<p style="text-align:center; margin-bottom:15px; margin-top:20px; font-size: 18px;"> 
					<a href="https://github.com/GuoBo98/Shape-Aware-Shipdet" style="color: hsl(17, 100%, 40%)"> Data and code coming soon!
					</a>
				</p>
		
			
			<tr>
				<!-- <td colspan="1"> -->
					<h2 class="add-top-margin">Abstract</h2>
					<hr>
				<!-- </td> -->
			</tr>
			<tr>
					<!-- <td colspan="1"> -->

				<p class="text" style="text-align:justify;">
					Fine-grained ship detection is a significant task in high-resolution remote sensing images. 
					However, large aspect ratios and severe category imbalance make fine-grained ship detection a challenging task. 
					Current general feature extraction methods are not entirely suitable for ships with large aspect ratios, 
					which will lead to misalignments in feature extraction, affecting the succeeding localization and classification. 
					For this problem, we propose a shape-aware feature learning method to fix the misalignments during feature extraction. 
					And For the problem of category imbalance, we design a ship shape-aware instance switching to balance the quantity distribution 
					of ships in different categories. It can ensure the consistency of the ocean background and ship direction when replacing 
					common ship instances with rare ship instances, which can improve the network's learning ability for rare instances. To verify 
					the effectiveness of the proposed method, we construct a multi-category ship detection dataset (MCSD) that contains 4000 images 
					carefully labeled with oriented bounding boxes, including 16 types of ship objects and nearly 18,000 instances. 
					We conduct experiments on MCSD and ShipRSImageNet, and extensive experiments indicate our proposed method's superiority for 
					ship detection.
				</p>
					<!-- </td> -->
			</tr>

			<tr>
				<!-- <td colspan="1"> -->
					<h2 class="add-top-margin">Introduction</h2>
					<hr>
				<!-- </td> -->
			</tr>

			<tr>
				<p class="text" style="text-align:justify;">
					Fine-grained ship detection in high-resolution remote sensing images is important for marine monitoring and management, 
					maritime rescue and coastal military defense early warning, etc. 
					In recent years, the resolution of remote sensing images is increasing with the development of earth observation technology, 
					which provides strong support for fine-grained ship detection and recognition.
					

          <br>
	  <br>
					Benefiting from the powerful feature representation of deep convolutional neural network (DCNN), object detection algorithms based on DCNN, 
					such as Faster R-CNN, YOLO, and SSD have achieved excellent detection results in natural scene images. 
					However, remote sensing images are different from natural scene images in terms of imaging angle and resolution. The massive variations in the scale and orientation 
					of objects caused by the bird's-eye view bring challenges to remote sensing object detection. As the essential target in optical remote sensing 
					images, ship has the characteristics of arbitrary direction, large aspect ratio, and multiple categories, which lead to the following problems for ship detection in 
					remote sensing images.
 
          <br>
          <br>
            <b>Large aspect ratio</b>: Different from common objects, ships in remote sensing images have large aspect ratios. 
					The long and short edges of the ship need different size of receptive fields and feature representation parameters. 
					However, current feature extraction methods obtain square-like features on a single-level feature map. 
					Single-level feature map cannot provide multi-scale receptive fields, and the square-like RoI Align treats the long and short 
					edges equally, wasting a lot of parameters on the representation of short edge.
				
					
          <br>
          <br>
            <b>Severe Category Imbalance</b>: The number of ship instances of different categories varies greatly and presents a long-tail distribution, 
					leading to severe quantity imbalance in different categories.
					The instance amount of particular ships is small and model does not pay much attention to these categories during the training 
					and learning, which will degrade the performance of detector.
					
	  <br>
	  <br>
					In this paper, we design a novel ship detection network with Shape-aware feature learning and Shape-aware Instance Switching to solve the above problems. The Shape-aware feature learning workflow can be seen from Figure 1. and Figure 2. shows the visualization of Shape-aware Instance Switching.  
					To verify the effectiveness of our method, we construct a high-resolution optical multi-category ship detection dataset (MCSD), 
					in which ships are precisely labeled with oriented bounding boxes (OBB). Our contributions can be summarized as follows.
					
			<ul style="font-weight:normal; text-align:justify;">
					<li> We design a shape-aware feature learning method that contains Shape-aware RoI Extractor (SRE) and Shape-aware Rotated RoI Align (SRRA). It performs RoI feature extraction at each level of FPN and fuses these features based on the attention mechanism. It also introduces the shape-aware rotate RoI Align to adapt the shape of the ship, which fixes the problem of feature misalignment during feature extraction. </li>
					<li> We design a Shape-aware Instance Switching (SIS) to solve the problem of quantity imbalance in different categories. It ensures the consistency of the ocean background and ship direction during replacing common ship instances with rare ship instances. </li>
					<li> We contribute a high-resolution optical multi-category ship detection dataset (MCSD) with high-quality OBB annotation, expanding the size of the existing multi-class ship dataset.</li>							
			</ul>				

				</p>
			</tr>
	

					<div>
					<img class="center" src="./images/SRE.png" width="800" /> <br>
					<p class="image-caption">
						Figure 1. The workflow of the shape-aware feature extraction. For each RoI generated by the RPN, instead of just using the single-level FPN feature, shape-aware rotate RoI align is performed on the feature map of each level of the FPN. Finally, the extracted multi-level features are accumulated together, followed by a spatial attention module to weight the fused features to guide the learning of the detector.
					</p>				
					</div>
	


			<tr>
				<td>
					<div>
					<img class="center" src="./images/switch.png" width="1000" /> <br>
					<p class="image-caption">
						Figure 2. Visualization of ship shape aware instance switching. (b) shows the copy and paste strategy that is unable to maintain the ship direction consistency (c) illustrates the result of direct switching that leads to obvious background noise. (d) shows the result of our proposed method. It ensures the consistency of ship direction and background.
					</p>				
					</div>
				</td>
			</tr>
			
<!-- 			<tr>
				<td>
				<p class="text" style="text-align:justify;">
					  Based on the above, we design a multi-stage pipeline for detecting oil tanks in SAR images with the enhancement of optical images, as shown in Figure. 2. 
					  First, Optical-guided self-supervised learning is used in the pre-training stage to obtain the feature extractor suitable for SAR images. 
					  Then, in the training stage,  a teacher-student network is constructed to make the SAR branch learn how to recognize oil tanks from the optical branch through knowledge distillation. 
					  Finally, the model detects the oil tanks using only SAR images as input during the inference stage. Our contribution can be summarized as follows:
					
					<ul style="font-weight:normal; text-align:justify;">
							<li> We propose a pipeline that uses optical images to guide SAR images to train a better oil tank detector in SAR images. Paired optical images and SAR images are used in the training stage, while only SAR images are used in the inference stage. </li>
							<li> We introduce paired optical-SAR images into the self-supervised learning to obtain a better SAR image feature extractor, improving the downstream oil tank detection task by 8-9 %. </li>
							<li> We construct an oil tank detection dataset, Shape-Aware-Shipdet, to explore the effectiveness of the proposed method. The dataset contains 820 paired SAR-Optical images with carefully labeled bounding boxes.</li>							
					</ul>

				</p>
				</td>
			</tr> -->
      <!-- <tr>
				<td>
						<h2 class="add-top-margin">AI-TOD-v2</h2>
						<hr>
				</td>
			</tr>
      <tr>
        <td>
            <h3 class="add-top-margin">Statistics</h3>
            
        </td>
    </tr>

    <tr>
      <td>
        <div>
        <img class="center" src="images/instance-switch.PNG" width="1000" /> <br>
        <p class="image-caption">
          Figure 2. Statistics of classes and instances in AI-TOD-v2. (a) Histogram of the number of instances per class. (b) Histogram of number of instances per image. (c) Histogram of number of instances’ sizes. (d) Boxplot depicting the range of sizes for each object category.
        </p>				
        </div>
      </td>
    </tr>

	<tr>
		<td>
		  <div>
		  <img class="center" src="images/mean.PNG" width="500" /> <br>
		  <p class="image-caption">
			Table 1. Mean and standard deviation of object scale on different
			datasets.
		  </p>				
		  </div>
		</td>
	  </tr>

	  <tr>
        <td>
            <h3 class="add-top-margin">Download</h3>
            1. AI-TOD
			[<a href="https://drive.google.com/drive/folders/1mokzFtLCjyqalSEajYTUmyzXvOHAa4WX" style="color: #0088CC">images & annotations</a>]<br>
			2. AI-TOD-v2
			[<a href="https://drive.google.com/drive/folders/1mokzFtLCjyqalSEajYTUmyzXvOHAa4WX" style="color: #0088CC">images & annotations</a>]<br>
			Note that the AI-TOD and AI-TOD-v2 share the image sets.
        </td>
     </tr> -->

			<tr>
				<td>
						<h2 class="add-top-margin">Experimental Results</h2>
						<hr>
				</td>
			</tr>

			<tr>
					<td>
							<h3 class="add-top-margin">A Comparison of Baseline on Ship Detection Dataset</h3>
							
					</td>
			</tr>
					
			<tr>
				<td>
					<div>
					<img class="center" src="images/vis.png" width="1000" /> <br>
					<p class="image-caption">
						Figure 3. Visualization of ship detection results. (a), (c) and (e) are the results of baseline. (b), (d) and (f) are the results of our proposed method. The green bounding box indicates the True Positive, blue bounding box indicates the False Negative, red bounding box indicates the False Negative, and the yellow bounding box indicates the right location but wrong classification.
					</p>				
					</div>
				</td>
			</tr>

			<!-- <tr>
				<td>
						<h2 class="add-top-margin">Related Work</h2>
						<hr>
						<p class="text" style="text-align:justify;"></p>
						Our work is built based on our past works including: <a href="https://ieeexplore.ieee.org/abstract/document/9413340" style="color: #0088CC">AI-TOD</a> [1], <a href="https://arxiv.org/abs/2102.12219" style="color: #0088CC">DOTA-v2.0</a> [3], <a href="https://openaccess.thecvf.com/content/CVPR2021W/EarthVision/html/Xu_Dot_Distance_for_Tiny_Object_Detection_in_Aerial_Images_CVPRW_2021_paper.html" style="color: #0088CC">DotD</a> [4] and <a href="https://arxiv.org/abs/2110.13389" style="color: #0088CC">NWD</a> [5]. If you found our work helpful, consider citing these works as well.
				</td>
			</tr> -->
			<tr>
					<td>
							<h2 class="add-top-margin">Acknowledgements</h2>
							<hr>
							<p class="text" style="text-align:justify;"></p>
						We would like to thank the anonymous reviewers for their valuable comments and contributions.  
						The numerical calculations in this article have been done on the supercomputing system in the Supercomputing Center, Wuhan University.
							
					</td>
				</tr>

			<tr>
					<td colspan="2">
					<h2 class="add-top-margin">References</h2>
					<hr>
					<ol style="padding-inline-start:20px;">
						<li>
							<b>ShipRSImageNet: A Large-Scale Fine-Grained Dataset for Ship Detection in High-Resolution Optical Remote Sensing Images </b>
							[<a href="https://ieeexplore.ieee.org/abstract/document/9512396" style="color: #0088CC">paper</a>]<br>
								Zhang, Zhengning and Zhang, Lin and Wang, Yue and Feng, Pengming and He, Ran
							et al. <br>
								IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (<b>JSTAR</b>), 2021.
						</li>
						<li>
							<b>Object detection in aerial images: A large-scale benchmark and challenges} </b>
							[<a href="https://ieeexplore.ieee.org/abstract/document/9560031" style="color: #0088CC">paper</a>]<br>
								Ding, Jian and Xue, Nan and Xia, Gui-Song and Bai, Xiang and Yang, Wen and Yang, Michael and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and others. <br>
								IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021.
						</li>	
						<li>
							<b>Oriented r-cnn for object detection </b>
							[<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Xie_Oriented_R-CNN_for_Object_Detection_ICCV_2021_paper.html" style="color: #0088CC">paper</a>]<br>
								Xie, Xingxing and Cheng, Gong and Wang, Jiabao and Yao, Xiwen and Han, Junwei. <br>
								Proceedings of the IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2021.
						</li>	

						<li>
							<b>Learning roi transformer for oriented object detection in aerial images </b>
							[<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Ding_Learning_RoI_Transformer_for_Oriented_Object_Detection_in_Aerial_Images_CVPR_2019_paper.html" style="color: #0088CC">paper</a>]<br>
							Ding, Jian and Xue, Nan and Long, Yang and Xia, Gui-Song and Lu, Qikai. <br>
							Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.
						</li>	

						

					</ol>
					</td>
				</tr>
				
					
			<br><br>
		 </table>
		 			
		
	<div class="footer">
		<p class="block">&copy; 2021 by Chang Xu at SPL</p>
	</div>

	</div>
</div>
</body>
</html>
